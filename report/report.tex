\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{Ling Fei Zhang, Brandon Ma}
\lhead{Artificial Intelligence}
\cfoot{\thepage}
\author{Ling Fei Zhang, 260985358\\
Brandon Ma, 260983550}
\begin{document}
\title{AI Final Project Report Winter 2022}    
\maketitle


\section{Motivation}
\subsection{Approach}
\paragraph{March 25th 2022}
Today, we played a few games against each other. We believe that in order to design a good AI agent, 
we must first understand the fundamentals of the game as well as to develop a few basic strategies. 
Our first instinct is to avoid 
corners and borders as well as being near a lot of walls. Our main strategy is to limit the opponent's 
movement while allowing ourselves to have a lot of moves. This means that generally speaking, we would 
like to move towards the center and block the opponent. 


\section{Theoretical Approach}
\subsection{Basic Strategies}
\paragraph{} First, we need to implement some basic strategies. We implemented a Depth First Search in order to enumerate 
all the possible moves given a specific position, a chess board and a maximum number of steps. Below we 
have some other basic strategies, we are using a heuristic that is considering the following points at every turn:
\begin{enumerate}
    \item Check how close we are to the center. The closer we are, the more points we award to our agent. 
    In parallel, we also evaluate how close the opponent is to the center. The heuristic score is given by 
    our agent's position relative to the opponent's position towards the center. 
    \item  Check the number of walls surrounding both our agent and the opponent. The distance to consider 
    is $n = \lfloor\frac{\text{Board Size}}{4}\rfloor$. The heuristic score is given by 
    the number of walls around our agent relative to the number of 
    walls around the opponent. 
    \item Check how many moves our agent and the opponent can make. The heuristic score is given by the 
    number of moves our agent has relative to the number of moves the opponent has. 
\end{enumerate}
\paragraph{} The algorithm that we have chosen to implement is Minimax with Alpha-Beta Pruning. 
The goal is to use a heuristic function to give a score to each move. This implies that the agent can 
decide which move has better winning chances relative to others. The leaf nodes that we get from the Alpha-Beta 
algorithm are positions that are evaluated according to the three points above, each having a different weight.
We have decided to make our agent more aggressive, thus we are giving a higher scaling factor to limiting the 
opponent's moves. To help with Alpha-Beta Pruning, we sort the first set of moves in order of closeness to the center.
\paragraph{} The way we have chosen to perform our search is using Iterative Deepening Search. The algorithm will try to reach the 
highest depth possible within 2 seconds. If it times out, it will take the best move of the previous depth search. 

\section{Advantages and Disadvantages}
\subsection{Advantages}
\paragraph{} Since we let the algorithm run for the entire 2 seconds, we are able to reach high depth with small size boards. 
Our program is always able to find mate in 1's, or a forced mate and we'll avoid moves that get ourselves 
checkmated whenever possible. 
It also favors centralized play, which reduces chances of getting cornered. 
\subsection{Disadvantages And Weaknesses}
\paragraph{} The algorithm doesn't prune as much as we expected. Hence, at higher board sizes, it is only able to reach a maximum 
depth of 1 in most cases. Furthermore, when the opponent is out of the range of our agent's moves, it will play extremely 
passively, as it does not know what to do. Due to the nature of Minimax, our agent assumes that the opponent will play 
optimally. Thus, if the program sees that the opponent can force us into a mate, it will view all 
moves as a loss and boxes itself. 

\section{Other Approaches}
\paragraph{} We've tried other approaches at the beginning such as using a Breadth First Search in order to generate the 
tree of possible moves. We realized that at each iteration of a Breadth First Search, 
we would also have to keep track of 
all the nodes up to the depth that we are searching for. On the other hand, an Iterative Deepening Search will not 
have to keep all of them in memory since either some nodes are already visited or they are nodes that we will 
visit later. Not to mention, Iterative Deepening Search also does better in our situation since we have a time 
constraint. In the event that we reach the time limit, the algorithm will still 
have some sense of what a good score is even without 
a full traversal of the desired depth. 
\paragraph{} After discovering our flaws with our Minimax algorithm, we attempted at removing moves that were futile, 
such as moving towards the edge of the board during the start of the game. 
When we exceeded a certain amount of possible moves, we were hoping that this would reduce the 
search space of our program. However, it led to a few errors, which resulted in the agent performing random walks at 
times. 
This was likely due to removing moves that could've been the only escape of the agent. It was also difficult to 
determine an appropriate cutoff value to determine when we would perform this elimination 
of moves. 


\section{Further Improvements}
\paragraph{} Considering that we are running a Minimax Algorithm with Alpha-Beta Pruning, a good improvement to our 
program is to sort the list of possible moves from best to worst before giving it to the algorithm. 
This way, the pruning can 
be more efficient, and the bad moves can be eliminated quicker. 
\paragraph{} Another improvement can be to not set an absolute depth for each branch of possible moves. 
For example, since the algorithm is limited in how far it can look ahead, it is very possible that it misses 
a sequenece of moves that can occur at leaf nodes. For example, the agent might want to avoid moving closer 
to the opponent because it forces the agent to put a wall "behind", and closing off an area of squares. 
If the agent only realizes that he loses a good portion of the squares without knowing that this move  
wins the game or puts the opponent at a great disadvantage, then it will likely not play it. 
A way to get around this is to perform a second round 
of limited search on the leaf nodes that are "unstable" (like the example above, or any move that puts 
the agent at an uncomfortable position, but that disadvantages the opponent even more). This way, 
it resolves the issue in a more dynamic way, and also makes the heuristic more accurate. 













\end{document}
